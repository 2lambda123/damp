{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_federated as tff\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do this if you are running on local jupyter -> https://github.com/tensorflow/federated/issues/842\n",
    "!pip install nest_asyncio\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = keras.utils.get_file(\"auto-mpg.data\", \"http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data\")\n",
    "\n",
    "column_names = ['MPG','Cylinders','Displacement','Horsepower','Weight', 'Acceleration', 'Model Year', 'Origin']\n",
    "\n",
    "raw_dataset = pd.read_csv(dataset_path, names=column_names,\n",
    "                      na_values = \"?\", comment='\\t',\n",
    "                      sep=\" \", skipinitialspace=True)\n",
    "\n",
    "df = raw_dataset.copy()\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.pairplot(df[[\"MPG\", \"Cylinders\", \"Displacement\", \"Weight\", \"Origin\"]], diag_kind=\"kde\", hue='Origin', \n",
    "             plot_kws = {'alpha': 0.6, 's': 80, 'edgecolor': 'k'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_be_normalized_columns = ['Cylinders', 'Displacement', 'Horsepower', 'Weight',\n",
    "                            'Acceleration', 'Model Year']\n",
    "\n",
    "target_column = \"MPG\"\n",
    "\n",
    "standard_scaler_x = StandardScaler(with_mean=True, with_std=True)\n",
    "df[to_be_normalized_columns + [target_column]\n",
    "   ] = standard_scaler_x.fit_transform(df[to_be_normalized_columns + [target_column]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "NUM_EPOCHS = 5\n",
    "BATCH_SIZE = 20\n",
    "SHUFFLE_BUFFER = 100\n",
    "PREFETCH_BUFFER = 10\n",
    "\n",
    "def preprocess(dataset):\n",
    "\n",
    "  def batch_format_fn(element):\n",
    "      return collections.OrderedDict(x=element['x'], y=element['y'])\n",
    "\n",
    "  return dataset.repeat(NUM_EPOCHS).shuffle(SHUFFLE_BUFFER).batch(\n",
    "      BATCH_SIZE).map(batch_format_fn).prefetch(PREFETCH_BUFFER)\n",
    "\n",
    "# produce datasets for each origin\n",
    "def make_federated_data():\n",
    "    dfs = [x for _, x in df.groupby('Origin')]\n",
    "    train_datasets = []\n",
    "    test_datasets = []\n",
    "\n",
    "    for dataframe in dfs:\n",
    "        target = dataframe.pop('MPG')\n",
    "        dataframe.pop(\"Origin\")\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(dataframe.values,\n",
    "                                                            target.values,\n",
    "                                                            test_size=0.2,\n",
    "                                                            random_state=42)\n",
    "\n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "            ({'x': X_train, 'y': y_train}))\n",
    "        \n",
    "        test_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "            ({'x': X_test, 'y': y_test}))\n",
    "\n",
    "        preprocessed_train_dataset = preprocess(train_dataset)\n",
    "        preprocessed_test_dataset = preprocess(test_dataset)\n",
    "\n",
    "        train_datasets.append(preprocessed_train_dataset)\n",
    "        test_datasets.append(preprocessed_test_dataset)\n",
    "        \n",
    "    return train_datasets, test_datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datasets, test_datasets = make_federated_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "  model = keras.Sequential([\n",
    "    layers.Dense(64, activation='relu', input_shape=[6]),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(1)\n",
    "  ])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tff_model():\n",
    "  return tff.learning.from_keras_model(build_model(), \n",
    "                                       input_spec=train_datasets[0].element_spec,\n",
    "                                       loss=tf.keras.losses.MeanSquaredError(),\n",
    "                                       metrics=[tf.keras.metrics.MeanSquaredError()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Create averaging process\")\n",
    "iterative_process = tff.learning.build_federated_averaging_process(model_fn=create_tff_model,\n",
    "                                                                   client_optimizer_fn = lambda: tf.keras.optimizers.SGD(0.002))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Initzialize averaging process\")\n",
    "state = iterative_process.initialize()\n",
    "\n",
    "print(\"Start iterations\")\n",
    "for _ in range(10):\n",
    "  state, metrics = iterative_process.next(state, train_datasets)\n",
    "  print('metrics={}'.format(metrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global model evaluated over all clients\n",
    "evaluation = tff.learning.build_federated_evaluation(model_fn=create_tff_model)\n",
    "test_metrics = evaluation(state.model, test_datasets)\n",
    "print(test_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global model evaluated per individual client\n",
    "for i in range(len(test_datasets)):\n",
    "    test_metrics = evaluation(state.model, [test_datasets[i]])\n",
    "    print(test_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
